{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_dense_sentiment_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiWM+VBDiPNFAlpk45rIbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishwvir-singh/DeepLearningForNLP/blob/main/keras_dense_sentiment_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjrXM-SP7AcK",
        "outputId": "c6521e97-eb12-48f1-fb20-86078d63df79"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets.imdb import load_data\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WImuPKX7KLj",
        "outputId": "f621f8de-6f9c-4597-a630-784ac731c156"
      },
      "source": [
        "import os\n",
        "sentiment_classifier_dir = '/content/sentiment_classifier'\n",
        "if not os.path.exists(sentiment_classifier_dir):\n",
        "    os.makedirs(sentiment_classifier_dir)\n",
        "    print('Directory created successfully !!')\n",
        "os.chdir(sentiment_classifier_dir)\n",
        "print('Path ---> ', os.getcwd(), 'ListDirs ---> ' , os.listdir())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory created successfully !!\n",
            "Path --->  /content/sentiment_classifier ListDirs --->  []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx7jF1xVDOXd"
      },
      "source": [
        "#vector space encoding\n",
        "n_unique_word = 5000 #pick only 5000 words from dataset\n",
        "skip_n_most_occur_word = 50\n",
        "max_word_limit = 100\n",
        "pad_type = trun_type = 'pre'\n",
        "\n",
        "#Training\n",
        "emd_dim = 64\n",
        "n_dense = 64\n",
        "dropout_value = 0.5\n",
        "epoch = 4\n",
        "batch_size = 128"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrR8grdi8r4g",
        "outputId": "8a7d2156-bf48-4d15-c427-c4bea76593fb"
      },
      "source": [
        "imdb_dataset_actual = load_data()\n",
        "imdb_dataset = load_data(num_words=n_unique_word, skip_top=skip_n_most_occur_word)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ncQ5H007KNn"
      },
      "source": [
        "(x_train, y_train), (x_valid, y_valid) = imdb_dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge0NxPqM7KRg",
        "outputId": "68551d5d-eeee-44ce-f980-cf6c7d5c188b"
      },
      "source": [
        "print(x_train[0:6]) # 0 reserved for padding; 1 would be starting character; 2 is unknown; 3 is most common word, etc."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([2, 2, 2, 2, 2, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 2, 173, 2, 256, 2, 2, 100, 2, 838, 112, 50, 670, 2, 2, 2, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 2, 2, 172, 4536, 1111, 2, 546, 2, 2, 447, 2, 192, 50, 2, 2, 147, 2025, 2, 2, 2, 2, 1920, 4613, 469, 2, 2, 71, 87, 2, 2, 2, 530, 2, 76, 2, 2, 1247, 2, 2, 2, 515, 2, 2, 2, 626, 2, 2, 2, 62, 386, 2, 2, 316, 2, 106, 2, 2, 2223, 2, 2, 480, 66, 3785, 2, 2, 130, 2, 2, 2, 619, 2, 2, 124, 51, 2, 135, 2, 2, 1415, 2, 2, 2, 2, 215, 2, 77, 52, 2, 2, 407, 2, 82, 2, 2, 2, 107, 117, 2, 2, 256, 2, 2, 2, 3766, 2, 723, 2, 71, 2, 530, 476, 2, 400, 317, 2, 2, 2, 2, 1029, 2, 104, 88, 2, 381, 2, 297, 98, 2, 2071, 56, 2, 141, 2, 194, 2, 2, 2, 226, 2, 2, 134, 476, 2, 480, 2, 144, 2, 2, 2, 51, 2, 2, 224, 92, 2, 104, 2, 226, 65, 2, 2, 1334, 88, 2, 2, 283, 2, 2, 4472, 113, 103, 2, 2, 2, 2, 2, 178, 2])\n",
            " list([2, 194, 1153, 194, 2, 78, 228, 2, 2, 1463, 4369, 2, 134, 2, 2, 715, 2, 118, 1634, 2, 394, 2, 2, 119, 954, 189, 102, 2, 207, 110, 3103, 2, 2, 69, 188, 2, 2, 2, 2, 2, 249, 126, 93, 2, 114, 2, 2300, 1523, 2, 647, 2, 116, 2, 2, 2, 2, 229, 2, 340, 1322, 2, 118, 2, 2, 130, 4901, 2, 2, 1002, 2, 89, 2, 952, 2, 2, 2, 455, 2, 2, 2, 2, 1543, 1905, 398, 2, 1649, 2, 2, 2, 163, 2, 3215, 2, 2, 1153, 2, 194, 775, 2, 2, 2, 349, 2637, 148, 605, 2, 2, 2, 123, 125, 68, 2, 2, 2, 349, 165, 4362, 98, 2, 2, 228, 2, 2, 2, 1157, 2, 299, 120, 2, 120, 174, 2, 220, 175, 136, 50, 2, 4373, 228, 2, 2, 2, 656, 245, 2350, 2, 2, 2, 131, 152, 491, 2, 2, 2, 2, 1212, 2, 2, 2, 371, 78, 2, 625, 64, 1382, 2, 2, 168, 145, 2, 2, 1690, 2, 2, 2, 1355, 2, 2, 2, 52, 154, 462, 2, 89, 78, 285, 2, 145, 95])\n",
            " list([2, 2, 2, 2, 2, 2, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 2, 71, 149, 2, 2, 112, 2, 2401, 311, 2, 2, 3711, 2, 75, 2, 1829, 296, 2, 86, 320, 2, 534, 2, 263, 4821, 1301, 2, 1873, 2, 89, 78, 2, 66, 2, 2, 360, 2, 2, 58, 316, 334, 2, 2, 1716, 2, 645, 662, 2, 257, 85, 1200, 2, 1228, 2578, 83, 68, 3912, 2, 2, 165, 1539, 278, 2, 69, 2, 780, 2, 106, 2, 2, 1338, 2, 2, 2, 2, 215, 2, 610, 2, 2, 87, 326, 2, 2300, 2, 2, 2, 2, 272, 2, 57, 2, 2, 2, 2, 2, 2, 2307, 51, 2, 170, 2, 595, 116, 595, 1352, 2, 191, 79, 638, 89, 2, 2, 2, 2, 106, 607, 624, 2, 534, 2, 227, 2, 129, 113])\n",
            " list([2, 2, 2, 2, 2, 2804, 2, 2040, 432, 111, 153, 103, 2, 1494, 2, 70, 131, 67, 2, 61, 2, 744, 2, 3715, 761, 61, 2, 452, 2, 2, 985, 2, 2, 59, 166, 2, 105, 216, 1239, 2, 1797, 2, 2, 2, 2, 744, 2413, 2, 2, 2, 687, 2, 2, 2, 2, 2, 3693, 2, 2, 2, 121, 59, 456, 2, 2, 2, 265, 2, 575, 111, 153, 159, 59, 2, 1447, 2, 2, 586, 482, 2, 2, 96, 59, 716, 2, 2, 172, 65, 2, 579, 2, 2, 2, 1615, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 464, 2, 314, 2, 2, 2, 719, 605, 2, 2, 202, 2, 310, 2, 3772, 3501, 2, 2722, 58, 2, 2, 537, 2116, 180, 2, 2, 413, 173, 2, 263, 112, 2, 152, 377, 2, 537, 263, 846, 579, 178, 54, 75, 71, 476, 2, 413, 263, 2504, 182, 2, 2, 75, 2306, 922, 2, 279, 131, 2895, 2, 2867, 2, 2, 2, 921, 2, 192, 2, 1219, 3890, 2, 2, 217, 4122, 1710, 537, 2, 1236, 2, 736, 2, 2, 61, 403, 2, 2, 2, 61, 4494, 2, 2, 4494, 159, 90, 263, 2311, 4319, 309, 2, 178, 2, 82, 4319, 2, 65, 2, 2, 145, 143, 2, 2, 2, 537, 746, 537, 537, 2, 2, 2, 2, 594, 2, 2, 94, 2, 3987, 2, 2, 2, 2, 538, 2, 1795, 246, 2, 2, 2, 2, 635, 2, 2, 51, 408, 2, 94, 318, 1382, 2, 2, 2, 2683, 936, 2, 2, 2, 2, 2, 2, 2, 1885, 2, 1118, 2, 80, 126, 842, 2, 2, 2, 2, 4726, 2, 4494, 2, 1550, 3633, 159, 2, 341, 2, 2733, 2, 4185, 173, 2, 90, 2, 2, 2, 2, 2, 1784, 86, 1117, 2, 3261, 2, 2, 2, 2, 2, 2, 2841, 2, 2, 1010, 2, 793, 2, 2, 1386, 1830, 2, 2, 246, 50, 2, 2, 2750, 1944, 746, 90, 2, 2, 2, 124, 2, 882, 2, 882, 496, 2, 2, 2213, 537, 121, 127, 1219, 130, 2, 2, 494, 2, 124, 2, 882, 496, 2, 341, 2, 2, 846, 2, 2, 2, 2, 1906, 2, 97, 2, 236, 2, 1311, 2, 2, 2, 2, 2, 2, 2, 91, 2, 3987, 70, 2, 882, 2, 579, 2, 2, 2, 2, 2, 537, 2, 2, 2, 2, 65, 2, 537, 75, 2, 1775, 3353, 2, 1846, 2, 2, 2, 154, 2, 2, 518, 53, 2, 2, 2, 3211, 882, 2, 399, 2, 75, 257, 3807, 2, 2, 2, 2, 456, 2, 65, 2, 2, 205, 113, 2, 2, 2, 2, 2, 2, 2, 242, 2, 91, 1202, 2, 2, 2070, 307, 2, 2, 2, 126, 93, 2, 2, 2, 188, 1076, 3222, 2, 2, 2, 2, 2348, 537, 2, 53, 537, 2, 82, 2, 2, 2, 2, 2, 280, 2, 219, 2, 2, 431, 758, 859, 2, 953, 1052, 2, 2, 2, 2, 94, 2, 2, 238, 60, 2, 2, 2, 804, 2, 2, 2, 2, 132, 2, 67, 2, 2, 2, 2, 283, 2, 2, 2, 2, 2, 242, 955, 2, 2, 279, 2, 2, 2, 1685, 195, 2, 238, 60, 796, 2, 2, 671, 2, 2804, 2, 2, 559, 154, 888, 2, 726, 50, 2, 2, 2, 2, 566, 2, 579, 2, 64, 2574])\n",
            " list([2, 249, 1323, 2, 61, 113, 2, 2, 2, 1637, 2, 2, 56, 2, 2401, 2, 457, 88, 2, 2626, 1400, 2, 3171, 2, 70, 79, 2, 706, 919, 2, 2, 355, 340, 355, 1696, 96, 143, 2, 2, 2, 289, 2, 61, 369, 71, 2359, 2, 2, 2, 131, 2073, 249, 114, 249, 229, 249, 2, 2, 2, 126, 110, 2, 473, 2, 569, 61, 419, 56, 429, 2, 1513, 2, 2, 534, 95, 474, 570, 2, 2, 124, 138, 88, 2, 421, 1543, 52, 725, 2, 61, 419, 2, 2, 1571, 2, 1543, 2, 2, 2, 2, 2, 296, 2, 3524, 2, 2, 421, 128, 74, 233, 334, 207, 126, 224, 2, 562, 298, 2167, 1272, 2, 2601, 2, 516, 988, 2, 2, 79, 120, 2, 595, 2, 784, 2, 3171, 2, 165, 170, 143, 2, 2, 2, 2, 2, 226, 251, 2, 61, 113])\n",
            " list([2, 778, 128, 74, 2, 630, 163, 2, 2, 1766, 2, 1051, 2, 2, 85, 156, 2, 2, 148, 139, 121, 664, 665, 2, 2, 1361, 173, 2, 749, 2, 2, 3804, 2, 2, 226, 65, 2, 2, 127, 2, 2, 2, 2])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w47VUJs77KTH",
        "outputId": "efff701f-e3a1-4901-f6b9-52028100f5ba"
      },
      "source": [
        "print(imdb_dataset_actual[0][0][0:6])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
            " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
            " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
            " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
            " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])\n",
            " list([1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 86588, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 15344, 10, 10])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcMqURca7KYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88d4836-1df5-41f5-d5a8-b149ef003e65"
      },
      "source": [
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "word_index = {k:v+3 for k,v in word_index.items()} #Manually handle 0, 1, 2 Indexes. Pad = 0, except n_unique_words word will change to 2, and 1 is used to show new sentence\n",
        "word_index['PAD'] = 0\n",
        "word_index['START'] = 1\n",
        "word_index['UKN'] = 2  ##Most occur word -- generally are use less i.e. the, is, a , etc\n",
        "#to convert number to word we need index to word mapping\n",
        "index_word = {v:k for k,v in word_index.items()}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz30aPkK7KaE",
        "outputId": "8a7328c7-0860-454a-a425-254cc9af7147"
      },
      "source": [
        " for sent in x_train[0:6]:\n",
        "    print([index_word[num] for num in sent])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', 'UKN', 'part', 'UKN', 'played', 'UKN', 'UKN', 'could', 'UKN', 'imagine', 'being', 'there', 'robert', 'UKN', 'UKN', 'UKN', 'amazing', 'actor', 'UKN', 'now', 'UKN', 'same', 'being', 'director', 'UKN', 'father', 'came', 'UKN', 'UKN', 'same', 'scottish', 'island', 'UKN', 'myself', 'UKN', 'UKN', 'loved', 'UKN', 'fact', 'there', 'UKN', 'UKN', 'real', 'connection', 'UKN', 'UKN', 'UKN', 'UKN', 'witty', 'remarks', 'throughout', 'UKN', 'UKN', 'were', 'great', 'UKN', 'UKN', 'UKN', 'brilliant', 'UKN', 'much', 'UKN', 'UKN', 'bought', 'UKN', 'UKN', 'UKN', 'soon', 'UKN', 'UKN', 'UKN', 'released', 'UKN', 'UKN', 'UKN', 'would', 'recommend', 'UKN', 'UKN', 'everyone', 'UKN', 'watch', 'UKN', 'UKN', 'fly', 'UKN', 'UKN', 'amazing', 'really', 'cried', 'UKN', 'UKN', 'end', 'UKN', 'UKN', 'UKN', 'sad', 'UKN', 'UKN', 'know', 'what', 'UKN', 'say', 'UKN', 'UKN', 'cry', 'UKN', 'UKN', 'UKN', 'UKN', 'must', 'UKN', 'been', 'good', 'UKN', 'UKN', 'definitely', 'UKN', 'also', 'UKN', 'UKN', 'UKN', 'two', 'little', 'UKN', 'UKN', 'played', 'UKN', 'UKN', 'UKN', 'norman', 'UKN', 'paul', 'UKN', 'were', 'UKN', 'brilliant', 'children', 'UKN', 'often', 'left', 'UKN', 'UKN', 'UKN', 'UKN', 'list', 'UKN', 'think', 'because', 'UKN', 'stars', 'UKN', 'play', 'them', 'UKN', 'grown', 'up', 'UKN', 'such', 'UKN', 'big', 'UKN', 'UKN', 'UKN', 'whole', 'UKN', 'UKN', 'these', 'children', 'UKN', 'amazing', 'UKN', 'should', 'UKN', 'UKN', 'UKN', 'what', 'UKN', 'UKN', 'done', \"don't\", 'UKN', 'think', 'UKN', 'whole', 'story', 'UKN', 'UKN', 'lovely', 'because', 'UKN', 'UKN', 'true', 'UKN', 'UKN', \"someone's\", 'life', 'after', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'us', 'UKN']\n",
            "['UKN', 'big', 'hair', 'big', 'UKN', 'bad', 'music', 'UKN', 'UKN', 'giant', 'safety', 'UKN', 'these', 'UKN', 'UKN', 'words', 'UKN', 'best', 'describe', 'UKN', 'terrible', 'UKN', 'UKN', 'love', 'cheesy', 'horror', 'movies', 'UKN', \"i've\", 'seen', 'hundreds', 'UKN', 'UKN', 'had', 'got', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'worst', 'ever', 'made', 'UKN', 'plot', 'UKN', 'paper', 'thin', 'UKN', 'ridiculous', 'UKN', 'acting', 'UKN', 'UKN', 'UKN', 'UKN', 'script', 'UKN', 'completely', 'laughable', 'UKN', 'best', 'UKN', 'UKN', 'end', 'showdown', 'UKN', 'UKN', 'cop', 'UKN', 'how', 'UKN', 'worked', 'UKN', 'UKN', 'UKN', 'killer', 'UKN', 'UKN', 'UKN', 'UKN', 'damn', 'terribly', 'written', 'UKN', 'clothes', 'UKN', 'UKN', 'UKN', 'funny', 'UKN', 'equal', 'UKN', 'UKN', 'hair', 'UKN', 'big', 'lots', 'UKN', 'UKN', 'UKN', 'men', 'wear', 'those', 'cut', 'UKN', 'UKN', 'UKN', 'show', 'off', 'their', 'UKN', 'UKN', 'UKN', 'men', 'actually', 'wore', 'them', 'UKN', 'UKN', 'music', 'UKN', 'UKN', 'UKN', 'trash', 'UKN', 'plays', 'over', 'UKN', 'over', 'again', 'UKN', 'almost', 'every', 'scene', 'there', 'UKN', 'trashy', 'music', 'UKN', 'UKN', 'UKN', 'taking', 'away', 'bodies', 'UKN', 'UKN', 'UKN', 'still', \"doesn't\", 'close', 'UKN', 'UKN', 'UKN', 'UKN', 'aside', 'UKN', 'UKN', 'UKN', 'truly', 'bad', 'UKN', 'whose', 'only', 'charm', 'UKN', 'UKN', 'look', 'back', 'UKN', 'UKN', 'disaster', 'UKN', 'UKN', 'UKN', \"80's\", 'UKN', 'UKN', 'UKN', 'good', 'old', 'laugh', 'UKN', 'how', 'bad', 'everything', 'UKN', 'back', 'then']\n",
            "['UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'worst', 'films', 'UKN', 'UKN', 'UKN', 'when', 'my', 'friends', 'UKN', 'were', 'watching', 'UKN', 'UKN', 'being', 'UKN', 'target', 'audience', 'UKN', 'UKN', 'aimed', 'UKN', 'we', 'UKN', 'sat', 'watched', 'UKN', 'first', 'half', 'UKN', 'hour', 'UKN', 'our', 'jaws', 'touching', 'UKN', 'floor', 'UKN', 'how', 'bad', 'UKN', 'really', 'UKN', 'UKN', 'rest', 'UKN', 'UKN', 'time', 'everyone', 'else', 'UKN', 'UKN', 'theatre', 'UKN', 'started', 'talking', 'UKN', 'each', 'other', 'leaving', 'UKN', 'generally', 'crying', 'into', 'their', 'popcorn', 'UKN', 'UKN', 'actually', 'paid', 'money', 'UKN', 'had', 'UKN', 'working', 'UKN', 'watch', 'UKN', 'UKN', 'excuse', 'UKN', 'UKN', 'UKN', 'UKN', 'must', 'UKN', 'looked', 'UKN', 'UKN', 'great', 'idea', 'UKN', 'paper', 'UKN', 'UKN', 'UKN', 'UKN', 'looks', 'UKN', 'no', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'clue', 'what', 'UKN', 'going', 'UKN', 'crap', 'acting', 'crap', 'costumes', 'UKN', \"can't\", 'get', 'across', 'how', 'UKN', 'UKN', 'UKN', 'UKN', 'watch', 'save', 'yourself', 'UKN', 'hour', 'UKN', 'bit', 'UKN', 'your', 'life']\n",
            "['UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'storytelling', 'UKN', 'traditional', 'sort', 'many', 'years', 'after', 'UKN', 'event', 'UKN', 'can', 'still', 'see', 'UKN', 'my', 'UKN', 'eye', 'UKN', 'elderly', 'lady', 'my', 'UKN', 'mother', 'UKN', 'UKN', 'battle', 'UKN', 'UKN', 'she', 'makes', 'UKN', 'characters', 'come', 'alive', 'UKN', 'passion', 'UKN', 'UKN', 'UKN', 'UKN', 'eye', 'witness', 'UKN', 'UKN', 'UKN', 'events', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'mile', 'UKN', 'UKN', 'UKN', 'where', 'she', 'lives', 'UKN', 'UKN', 'UKN', 'course', 'UKN', 'happened', 'many', 'years', 'before', 'she', 'UKN', 'born', 'UKN', 'UKN', \"wouldn't\", 'guess', 'UKN', 'UKN', 'way', 'she', 'tells', 'UKN', 'UKN', 'same', 'story', 'UKN', 'told', 'UKN', 'UKN', 'UKN', 'length', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'friend', 'UKN', 'night', 'UKN', 'UKN', 'UKN', 'local', 'cut', 'UKN', 'UKN', 'give', 'UKN', 'version', 'UKN', 'discussion', 'continued', 'UKN', 'closing', 'time', 'UKN', 'UKN', 'stories', 'passed', 'down', 'UKN', 'UKN', 'become', 'part', 'UKN', 'our', 'being', 'UKN', \"doesn't\", 'remember', 'UKN', 'stories', 'our', 'parents', 'told', 'us', 'when', 'we', 'were', 'children', 'UKN', 'become', 'our', 'invisible', 'world', 'UKN', 'UKN', 'we', 'grow', 'older', 'UKN', 'maybe', 'still', 'serve', 'UKN', 'inspiration', 'UKN', 'UKN', 'UKN', 'emotional', 'UKN', 'fact', 'UKN', 'fiction', 'blend', 'UKN', 'UKN', 'role', 'models', 'warning', 'stories', 'UKN', 'magic', 'UKN', 'mystery', 'UKN', 'UKN', 'my', 'name', 'UKN', 'UKN', 'UKN', 'my', 'grandfather', 'UKN', 'UKN', 'grandfather', 'before', 'him', 'our', 'protagonist', 'introduces', 'himself', 'UKN', 'us', 'UKN', 'also', 'introduces', 'UKN', 'story', 'UKN', 'UKN', 'back', 'through', 'UKN', 'UKN', 'UKN', 'stories', 'within', 'stories', 'stories', 'UKN', 'UKN', 'UKN', 'UKN', 'wonder', 'UKN', 'UKN', 'its', 'UKN', 'mountains', 'UKN', 'UKN', 'UKN', 'UKN', 'stuff', 'UKN', 'legend', 'yet', 'UKN', 'UKN', 'UKN', 'UKN', 'reality', 'UKN', 'UKN', 'what', 'gives', 'UKN', 'its', 'special', 'charm', 'UKN', 'UKN', 'UKN', 'rough', 'beauty', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'finest', 'UKN', 'singing', 'UKN', 'will', 'ever', 'hear', 'UKN', 'UKN', 'UKN', 'UKN', 'visits', 'UKN', 'grandfather', 'UKN', 'hospital', 'shortly', 'before', 'UKN', 'death', 'UKN', 'burns', 'UKN', 'frustration', 'part', 'UKN', 'him', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'twenty', 'first', 'century', 'UKN', 'hang', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'raised', 'UKN', 'UKN', 'western', 'UKN', 'among', 'UKN', 'UKN', 'speaking', 'community', 'UKN', 'UKN', 'yet', 'there', 'UKN', 'UKN', 'deeper', 'conflict', 'within', 'him', 'UKN', 'UKN', 'UKN', 'know', 'UKN', 'truth', 'UKN', 'truth', 'behind', 'UKN', 'UKN', 'ancient', 'stories', 'where', 'does', 'fiction', 'end', 'UKN', 'UKN', 'wants', 'UKN', 'know', 'UKN', 'truth', 'behind', 'UKN', 'death', 'UKN', 'UKN', 'parents', 'UKN', 'UKN', 'UKN', 'UKN', 'pulled', 'UKN', 'make', 'UKN', 'last', 'UKN', 'journey', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'most', 'UKN', 'mountains', 'can', 'UKN', 'truth', 'UKN', 'told', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'stories', 'UKN', 'UKN', 'UKN', 'UKN', 'story', 'UKN', 'stories', 'we', 'UKN', 'bloody', 'battles', 'UKN', 'lovers', 'UKN', 'UKN', 'UKN', 'old', 'UKN', 'UKN', 'sometimes', 'more', 'UKN', 'UKN', 'UKN', 'accepted', 'truth', 'UKN', 'doing', 'UKN', 'we', 'each', 'connect', 'UKN', 'UKN', 'UKN', 'UKN', 'lives', 'UKN', 'story', 'UKN', 'UKN', 'own', 'life', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'probably', 'UKN', 'most', 'honest', 'UKN', 'UKN', 'genuinely', 'beautiful', 'UKN', 'UKN', 'UKN', 'ever', 'made', 'UKN', 'UKN', 'UKN', 'got', 'slightly', 'annoyed', 'UKN', 'UKN', 'UKN', 'UKN', 'hanging', 'stories', 'UKN', 'more', 'stories', 'UKN', 'also', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'once', 'UKN', 'saw', 'UKN', 'UKN', 'picture', \"'\", 'forget', 'UKN', 'box', 'office', 'UKN', 'UKN', 'UKN', 'UKN', 'its', 'UKN', 'UKN', 'might', 'even', 'UKN', 'UKN', 'UKN', 'famous', 'UKN', 'UKN', 'UKN', 'UKN', 'man', 'UKN', 'see', 'UKN', 'UKN', 'UKN', 'UKN', 'true', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'probably', 'unique', 'UKN', 'UKN', 'maybe', 'UKN', 'UKN', 'UKN', 'deeply', 'enough', 'UKN', 'might', 'even', 're', 'UKN', 'UKN', 'power', 'UKN', 'storytelling', 'UKN', 'UKN', 'age', 'old', 'question', 'UKN', 'whether', 'there', 'UKN', 'UKN', 'UKN', 'UKN', 'cannot', 'UKN', 'told', 'UKN', 'only', 'experienced']\n",
            "['UKN', 'worst', 'mistake', 'UKN', 'my', 'life', 'UKN', 'UKN', 'UKN', 'picked', 'UKN', 'UKN', 'up', 'UKN', 'target', 'UKN', '5', 'because', 'UKN', 'figured', 'hey', 'UKN', 'sandler', 'UKN', 'can', 'get', 'UKN', 'cheap', 'laughs', 'UKN', 'UKN', 'wrong', 'completely', 'wrong', 'mid', 'way', 'through', 'UKN', 'UKN', 'UKN', 'three', 'UKN', 'my', 'friends', 'were', 'asleep', 'UKN', 'UKN', 'UKN', 'still', 'suffering', 'worst', 'plot', 'worst', 'script', 'worst', 'UKN', 'UKN', 'UKN', 'ever', 'seen', 'UKN', 'wanted', 'UKN', 'hit', 'my', 'head', 'up', 'against', 'UKN', 'wall', 'UKN', 'UKN', 'hour', 'then', \"i'd\", 'stop', 'UKN', 'UKN', 'know', 'why', 'because', 'UKN', 'felt', 'damn', 'good', 'upon', 'UKN', 'my', 'head', 'UKN', 'UKN', 'stuck', 'UKN', 'damn', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'watched', 'UKN', 'burn', 'UKN', 'UKN', 'felt', 'better', 'than', 'anything', 'else', \"i've\", 'ever', 'done', 'UKN', 'took', 'american', 'psycho', 'army', 'UKN', 'darkness', 'UKN', 'kill', 'bill', 'UKN', 'UKN', 'get', 'over', 'UKN', 'crap', 'UKN', 'hate', 'UKN', 'sandler', 'UKN', 'actually', 'going', 'through', 'UKN', 'UKN', 'UKN', 'UKN', 'UKN', 'whole', 'day', 'UKN', 'my', 'life']\n",
            "['UKN', 'begins', 'better', 'than', 'UKN', 'ends', 'funny', 'UKN', 'UKN', 'russian', 'UKN', 'crew', 'UKN', 'UKN', 'other', 'actors', 'UKN', 'UKN', 'those', 'scenes', 'where', 'documentary', 'shots', 'UKN', 'UKN', 'spoiler', 'part', 'UKN', 'message', 'UKN', 'UKN', 'contrary', 'UKN', 'UKN', 'whole', 'story', 'UKN', 'UKN', 'does', 'UKN', 'UKN', 'UKN', 'UKN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfSXChbo7KeW",
        "outputId": "69053e22-b8d2-454b-bef8-e80fe8ec3303"
      },
      "source": [
        "for sent in imdb_dataset_actual[0][0][0:6]:\n",
        "    print([index_word[num] for num in sent])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['START', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', \"redford's\", 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', \"norman's\", 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', 'retail', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', 'fishing', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', 'congratulations', 'to', 'the', 'two', 'little', \"boy's\", 'that', 'played', 'the', \"part's\", 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', 'praising', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', 'profile', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', 'praised', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', \"someone's\", 'life', 'after', 'all', 'that', 'was', 'shared', 'with', 'us', 'all']\n",
            "['START', 'big', 'hair', 'big', 'boobs', 'bad', 'music', 'and', 'a', 'giant', 'safety', 'pin', 'these', 'are', 'the', 'words', 'to', 'best', 'describe', 'this', 'terrible', 'movie', 'i', 'love', 'cheesy', 'horror', 'movies', 'and', \"i've\", 'seen', 'hundreds', 'but', 'this', 'had', 'got', 'to', 'be', 'on', 'of', 'the', 'worst', 'ever', 'made', 'the', 'plot', 'is', 'paper', 'thin', 'and', 'ridiculous', 'the', 'acting', 'is', 'an', 'abomination', 'the', 'script', 'is', 'completely', 'laughable', 'the', 'best', 'is', 'the', 'end', 'showdown', 'with', 'the', 'cop', 'and', 'how', 'he', 'worked', 'out', 'who', 'the', 'killer', 'is', \"it's\", 'just', 'so', 'damn', 'terribly', 'written', 'the', 'clothes', 'are', 'sickening', 'and', 'funny', 'in', 'equal', 'measures', 'the', 'hair', 'is', 'big', 'lots', 'of', 'boobs', 'bounce', 'men', 'wear', 'those', 'cut', 'tee', 'shirts', 'that', 'show', 'off', 'their', 'stomachs', 'sickening', 'that', 'men', 'actually', 'wore', 'them', 'and', 'the', 'music', 'is', 'just', 'synthesiser', 'trash', 'that', 'plays', 'over', 'and', 'over', 'again', 'in', 'almost', 'every', 'scene', 'there', 'is', 'trashy', 'music', 'boobs', 'and', 'paramedics', 'taking', 'away', 'bodies', 'and', 'the', 'gym', 'still', \"doesn't\", 'close', 'for', 'bereavement', 'all', 'joking', 'aside', 'this', 'is', 'a', 'truly', 'bad', 'film', 'whose', 'only', 'charm', 'is', 'to', 'look', 'back', 'on', 'the', 'disaster', 'that', 'was', 'the', \"80's\", 'and', 'have', 'a', 'good', 'old', 'laugh', 'at', 'how', 'bad', 'everything', 'was', 'back', 'then']\n",
            "['START', 'this', 'has', 'to', 'be', 'one', 'of', 'the', 'worst', 'films', 'of', 'the', '1990s', 'when', 'my', 'friends', 'i', 'were', 'watching', 'this', 'film', 'being', 'the', 'target', 'audience', 'it', 'was', 'aimed', 'at', 'we', 'just', 'sat', 'watched', 'the', 'first', 'half', 'an', 'hour', 'with', 'our', 'jaws', 'touching', 'the', 'floor', 'at', 'how', 'bad', 'it', 'really', 'was', 'the', 'rest', 'of', 'the', 'time', 'everyone', 'else', 'in', 'the', 'theatre', 'just', 'started', 'talking', 'to', 'each', 'other', 'leaving', 'or', 'generally', 'crying', 'into', 'their', 'popcorn', 'that', 'they', 'actually', 'paid', 'money', 'they', 'had', 'earnt', 'working', 'to', 'watch', 'this', 'feeble', 'excuse', 'for', 'a', 'film', 'it', 'must', 'have', 'looked', 'like', 'a', 'great', 'idea', 'on', 'paper', 'but', 'on', 'film', 'it', 'looks', 'like', 'no', 'one', 'in', 'the', 'film', 'has', 'a', 'clue', 'what', 'is', 'going', 'on', 'crap', 'acting', 'crap', 'costumes', 'i', \"can't\", 'get', 'across', 'how', 'embarrasing', 'this', 'is', 'to', 'watch', 'save', 'yourself', 'an', 'hour', 'a', 'bit', 'of', 'your', 'life']\n",
            "['START', 'the', 'scots', 'excel', 'at', 'storytelling', 'the', 'traditional', 'sort', 'many', 'years', 'after', 'the', 'event', 'i', 'can', 'still', 'see', 'in', 'my', \"mind's\", 'eye', 'an', 'elderly', 'lady', 'my', \"friend's\", 'mother', 'retelling', 'the', 'battle', 'of', 'culloden', 'she', 'makes', 'the', 'characters', 'come', 'alive', 'her', 'passion', 'is', 'that', 'of', 'an', 'eye', 'witness', 'one', 'to', 'the', 'events', 'on', 'the', 'sodden', 'heath', 'a', 'mile', 'or', 'so', 'from', 'where', 'she', 'lives', 'br', 'br', 'of', 'course', 'it', 'happened', 'many', 'years', 'before', 'she', 'was', 'born', 'but', 'you', \"wouldn't\", 'guess', 'from', 'the', 'way', 'she', 'tells', 'it', 'the', 'same', 'story', 'is', 'told', 'in', 'bars', 'the', 'length', 'and', 'breadth', 'of', 'scotland', 'as', 'i', 'discussed', 'it', 'with', 'a', 'friend', 'one', 'night', 'in', 'mallaig', 'a', 'local', 'cut', 'in', 'to', 'give', 'his', 'version', 'the', 'discussion', 'continued', 'to', 'closing', 'time', 'br', 'br', 'stories', 'passed', 'down', 'like', 'this', 'become', 'part', 'of', 'our', 'being', 'who', \"doesn't\", 'remember', 'the', 'stories', 'our', 'parents', 'told', 'us', 'when', 'we', 'were', 'children', 'they', 'become', 'our', 'invisible', 'world', 'and', 'as', 'we', 'grow', 'older', 'they', 'maybe', 'still', 'serve', 'as', 'inspiration', 'or', 'as', 'an', 'emotional', 'reservoir', 'fact', 'and', 'fiction', 'blend', 'with', 'aspiration', 'role', 'models', 'warning', 'stories', 'archetypes', 'magic', 'and', 'mystery', 'br', 'br', 'my', 'name', 'is', 'aonghas', 'like', 'my', 'grandfather', 'and', 'his', 'grandfather', 'before', 'him', 'our', 'protagonist', 'introduces', 'himself', 'to', 'us', 'and', 'also', 'introduces', 'the', 'story', 'that', 'stretches', 'back', 'through', 'generations', 'it', 'produces', 'stories', 'within', 'stories', 'stories', 'that', 'evoke', 'the', 'impenetrable', 'wonder', 'of', 'scotland', 'its', 'rugged', 'mountains', 'shrouded', 'in', 'mists', 'the', 'stuff', 'of', 'legend', 'yet', \"seach'd\", 'is', 'rooted', 'in', 'reality', 'this', 'is', 'what', 'gives', 'it', 'its', 'special', 'charm', 'it', 'has', 'a', 'rough', 'beauty', 'and', 'authenticity', 'tempered', 'with', 'some', 'of', 'the', 'finest', 'gaelic', 'singing', 'you', 'will', 'ever', 'hear', 'br', 'br', 'aonghas', 'angus', 'visits', 'his', 'grandfather', 'in', 'hospital', 'shortly', 'before', 'his', 'death', 'he', 'burns', 'with', 'frustration', 'part', 'of', 'him', 'yearns', 'to', 'be', 'in', 'the', 'twenty', 'first', 'century', 'to', 'hang', 'out', 'in', 'glasgow', 'but', 'he', 'is', 'raised', 'on', 'the', 'western', 'shores', 'among', 'a', 'gaelic', 'speaking', 'community', 'br', 'br', 'yet', 'there', 'is', 'a', 'deeper', 'conflict', 'within', 'him', 'he', 'yearns', 'to', 'know', 'the', 'truth', 'the', 'truth', 'behind', 'his', \"grandfather's\", 'ancient', 'stories', 'where', 'does', 'fiction', 'end', 'and', 'he', 'wants', 'to', 'know', 'the', 'truth', 'behind', 'the', 'death', 'of', 'his', 'parents', 'br', 'br', 'he', 'is', 'pulled', 'to', 'make', 'a', 'last', 'fateful', 'journey', 'to', 'the', 'summit', 'of', 'one', 'of', \"scotland's\", 'most', 'inaccessible', 'mountains', 'can', 'the', 'truth', 'be', 'told', 'or', 'is', 'it', 'all', 'in', 'stories', 'br', 'br', 'in', 'this', 'story', 'about', 'stories', 'we', 'revisit', 'bloody', 'battles', 'poisoned', 'lovers', 'the', 'folklore', 'of', 'old', 'and', 'the', 'sometimes', 'more', 'treacherous', 'folklore', 'of', 'accepted', 'truth', 'in', 'doing', 'so', 'we', 'each', 'connect', 'with', 'angus', 'as', 'he', 'lives', 'the', 'story', 'of', 'his', 'own', 'life', 'br', 'br', 'seachd', 'the', 'inaccessible', 'pinnacle', 'is', 'probably', 'the', 'most', 'honest', 'unpretentious', 'and', 'genuinely', 'beautiful', 'film', 'of', 'scotland', 'ever', 'made', 'like', 'angus', 'i', 'got', 'slightly', 'annoyed', 'with', 'the', 'pretext', 'of', 'hanging', 'stories', 'on', 'more', 'stories', 'but', 'also', 'like', 'angus', 'i', 'forgave', 'this', 'once', 'i', 'saw', 'the', \"'bigger\", 'picture', \"'\", 'forget', 'the', 'box', 'office', 'pastiche', 'of', 'braveheart', 'and', 'its', 'like', 'you', 'might', 'even', 'forego', 'the', 'justly', 'famous', 'dramatisation', 'of', 'the', 'wicker', 'man', 'to', 'see', 'a', 'film', 'that', 'is', 'true', 'to', 'scotland', 'this', 'one', 'is', 'probably', 'unique', 'if', 'you', 'maybe', 'meditate', 'on', 'it', 'deeply', 'enough', 'you', 'might', 'even', 're', 'evaluate', 'the', 'power', 'of', 'storytelling', 'and', 'the', 'age', 'old', 'question', 'of', 'whether', 'there', 'are', 'some', 'truths', 'that', 'cannot', 'be', 'told', 'but', 'only', 'experienced']\n",
            "['START', 'worst', 'mistake', 'of', 'my', 'life', 'br', 'br', 'i', 'picked', 'this', 'movie', 'up', 'at', 'target', 'for', '5', 'because', 'i', 'figured', 'hey', \"it's\", 'sandler', 'i', 'can', 'get', 'some', 'cheap', 'laughs', 'i', 'was', 'wrong', 'completely', 'wrong', 'mid', 'way', 'through', 'the', 'film', 'all', 'three', 'of', 'my', 'friends', 'were', 'asleep', 'and', 'i', 'was', 'still', 'suffering', 'worst', 'plot', 'worst', 'script', 'worst', 'movie', 'i', 'have', 'ever', 'seen', 'i', 'wanted', 'to', 'hit', 'my', 'head', 'up', 'against', 'a', 'wall', 'for', 'an', 'hour', 'then', \"i'd\", 'stop', 'and', 'you', 'know', 'why', 'because', 'it', 'felt', 'damn', 'good', 'upon', 'bashing', 'my', 'head', 'in', 'i', 'stuck', 'that', 'damn', 'movie', 'in', 'the', 'microwave', 'and', 'watched', 'it', 'burn', 'and', 'that', 'felt', 'better', 'than', 'anything', 'else', \"i've\", 'ever', 'done', 'it', 'took', 'american', 'psycho', 'army', 'of', 'darkness', 'and', 'kill', 'bill', 'just', 'to', 'get', 'over', 'that', 'crap', 'i', 'hate', 'you', 'sandler', 'for', 'actually', 'going', 'through', 'with', 'this', 'and', 'ruining', 'a', 'whole', 'day', 'of', 'my', 'life']\n",
            "['START', 'begins', 'better', 'than', 'it', 'ends', 'funny', 'that', 'the', 'russian', 'submarine', 'crew', 'outperforms', 'all', 'other', 'actors', \"it's\", 'like', 'those', 'scenes', 'where', 'documentary', 'shots', 'br', 'br', 'spoiler', 'part', 'the', 'message', 'dechifered', 'was', 'contrary', 'to', 'the', 'whole', 'story', 'it', 'just', 'does', 'not', 'mesh', 'br', 'br']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXWfSdHI7KgR",
        "outputId": "e55b571f-40d0-46d7-d3e7-ca67952f484c"
      },
      "source": [
        "#Preprocess Data\n",
        "\n",
        "x_train = pad_sequences(x_train, padding=pad_type, truncating=trun_type, maxlen=max_word_limit)\n",
        "x_valid = pad_sequences(x_valid, padding=pad_type, truncating=trun_type, maxlen=max_word_limit)\n",
        "print(imdb_dataset[0][0][0][-100:] == x_train[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz24mUGV7Kku",
        "outputId": "e6f48fdc-6cd8-42ea-dff0-3e0ca4aba55e"
      },
      "source": [
        "#Design Neural Network Achitecture\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_unique_word, output_dim=emd_dim, input_length=max_word_limit))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(n_dense, activation='relu'))\n",
        "model.add(Dropout(dropout_value))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 64)           320000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                409664    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 729,729\n",
            "Trainable params: 729,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sotVUdoZjgP0",
        "outputId": "f1dff2df-fe9b-4228-e5b1-7bc1ed9c0742"
      },
      "source": [
        "# embedding layer dimensions and parameters: \n",
        "print(emd_dim, n_unique_word, emd_dim*n_unique_word)\n",
        "\n",
        "# ...flatten:\n",
        "print(max_word_limit, emd_dim, emd_dim*max_word_limit)\n",
        "\n",
        "# ...dense:\n",
        "print(n_dense, emd_dim*max_word_limit*n_dense + n_dense) # weights + biases\n",
        "\n",
        "# ...and output:\n",
        "print(n_dense + 1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 5000 320000\n",
            "100 64 6400\n",
            "64 409664\n",
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5spbBoc6jgi_"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Nadam(), loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QanS2hAjgk0"
      },
      "source": [
        "modelcheckpoint = ModelCheckpoint(filepath= sentiment_classifier_dir + \"/weights.{epoch:02d}.hdf5\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYsmXdbW7Kmo",
        "outputId": "3d3c8519-2c71-4d38-ae7f-2e83841e6005"
      },
      "source": [
        "model.fit(x= x_train, y = y_train, batch_size= batch_size, epochs=epoch, verbose=1, validation_data=(x_valid, y_valid), callbacks=modelcheckpoint)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "196/196 [==============================] - 5s 12ms/step - loss: 0.6431 - accuracy: 0.5855 - val_loss: 0.3583 - val_accuracy: 0.8416\n",
            "Epoch 2/4\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.2889 - accuracy: 0.8853 - val_loss: 0.3484 - val_accuracy: 0.8472\n",
            "Epoch 3/4\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 0.1266 - accuracy: 0.9650 - val_loss: 0.4294 - val_accuracy: 0.8325\n",
            "Epoch 4/4\n",
            "196/196 [==============================] - 2s 12ms/step - loss: 0.0293 - accuracy: 0.9961 - val_loss: 0.5333 - val_accuracy: 0.8267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f927a281668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLLEGw-O7Kq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe89aa44-3da7-49c1-da02-c06366d36895"
      },
      "source": [
        "#Since you have used checkpoints then you should load the weights where you have received less val_loss and high val_accuracy. If val_loss and accuracy are increasing it means we are over fitting model.\n",
        "\n",
        "#so in here we can see that weights of epoch 2 are perfect for us.\n",
        "\n",
        "model.get_weights()  ###Default --- weights of final epoch"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.00381651, -0.00011605, -0.0033314 , ...,  0.01558343,\n",
              "         -0.01466183, -0.00638713],\n",
              "        [-0.04053509,  0.00023309,  0.03643062, ..., -0.01560362,\n",
              "         -0.02013558, -0.02105451],\n",
              "        [-0.00113827,  0.00857216, -0.00311051, ..., -0.00520972,\n",
              "         -0.01764137,  0.00343425],\n",
              "        ...,\n",
              "        [-0.01172521, -0.0309516 ,  0.03467025, ...,  0.02974921,\n",
              "          0.03312327, -0.02746096],\n",
              "        [-0.04850123,  0.02920745, -0.03188724, ...,  0.04044978,\n",
              "          0.03619353,  0.0307804 ],\n",
              "        [ 0.01877861, -0.07156576,  0.01293642, ..., -0.01828024,\n",
              "          0.09442347,  0.04749263]], dtype=float32),\n",
              " array([[ 0.01845437,  0.00428391, -0.01361202, ..., -0.01346045,\n",
              "          0.01902734, -0.0083605 ],\n",
              "        [-0.02076091,  0.04982424, -0.05624454, ..., -0.02994901,\n",
              "          0.01876197,  0.01271705],\n",
              "        [ 0.02665348,  0.02175783, -0.04030456, ..., -0.06602138,\n",
              "         -0.01848558, -0.00640472],\n",
              "        ...,\n",
              "        [ 0.10117906,  0.09985145, -0.01572288, ..., -0.1096158 ,\n",
              "         -0.05372269,  0.07710177],\n",
              "        [-0.02225287,  0.00566194, -0.05236516, ..., -0.00375262,\n",
              "          0.02528609, -0.0295279 ],\n",
              "        [ 0.00238946, -0.06446978, -0.021761  , ...,  0.07698505,\n",
              "         -0.00360333, -0.05767358]], dtype=float32),\n",
              " array([ 0.02826707,  0.02983469,  0.03467281,  0.02314784,  0.02702721,\n",
              "         0.03507429,  0.02164641,  0.02890937,  0.03380743,  0.01407846,\n",
              "         0.03259788,  0.02578565,  0.02241496,  0.01846202,  0.02056259,\n",
              "         0.02701131,  0.02073483,  0.01853512,  0.02213456,  0.04547918,\n",
              "         0.03373869, -0.01564341,  0.02499764,  0.02179848,  0.03072955,\n",
              "         0.01913   ,  0.02022431,  0.03990723,  0.03221491,  0.02292425,\n",
              "         0.01988939,  0.02370854,  0.0187361 ,  0.02069995,  0.02414114,\n",
              "         0.02071499,  0.03683196,  0.03914019, -0.01716228,  0.02295018,\n",
              "         0.02070753,  0.01695728,  0.0304428 ,  0.03096487,  0.02317715,\n",
              "         0.02409407,  0.02598557,  0.01923769,  0.0222316 ,  0.02442671,\n",
              "         0.03174033,  0.02046944,  0.04680178,  0.02310469,  0.02488754,\n",
              "        -0.02034032,  0.02855914,  0.02731226,  0.03067366,  0.01434584,\n",
              "         0.02190048,  0.02690697,  0.02448339,  0.02043771], dtype=float32),\n",
              " array([[ 0.40409374],\n",
              "        [ 0.2958768 ],\n",
              "        [-0.45771202],\n",
              "        [-0.39312485],\n",
              "        [-0.29395926],\n",
              "        [-0.21187286],\n",
              "        [ 0.1863521 ],\n",
              "        [-0.368548  ],\n",
              "        [-0.3369022 ],\n",
              "        [ 0.37720734],\n",
              "        [ 0.3458468 ],\n",
              "        [ 0.25928658],\n",
              "        [-0.43291175],\n",
              "        [-0.4870987 ],\n",
              "        [ 0.48938254],\n",
              "        [-0.2686237 ],\n",
              "        [-0.4220042 ],\n",
              "        [ 0.34359697],\n",
              "        [ 0.3742861 ],\n",
              "        [ 0.09831007],\n",
              "        [-0.24950768],\n",
              "        [ 0.09131911],\n",
              "        [ 0.48619863],\n",
              "        [ 0.31163818],\n",
              "        [-0.36311153],\n",
              "        [ 0.41368788],\n",
              "        [ 0.24811892],\n",
              "        [ 0.33512396],\n",
              "        [-0.422619  ],\n",
              "        [ 0.36380196],\n",
              "        [ 0.3537127 ],\n",
              "        [-0.29353014],\n",
              "        [-0.43984964],\n",
              "        [-0.41130513],\n",
              "        [-0.34362862],\n",
              "        [-0.41866586],\n",
              "        [ 0.40070638],\n",
              "        [ 0.3124344 ],\n",
              "        [-0.2785404 ],\n",
              "        [-0.50227326],\n",
              "        [ 0.44211838],\n",
              "        [ 0.52779824],\n",
              "        [ 0.34869248],\n",
              "        [ 0.29486662],\n",
              "        [ 0.29010305],\n",
              "        [-0.31168404],\n",
              "        [ 0.38152808],\n",
              "        [-0.50723773],\n",
              "        [-0.47587883],\n",
              "        [-0.4564627 ],\n",
              "        [ 0.26010782],\n",
              "        [ 0.37583363],\n",
              "        [-0.28995162],\n",
              "        [-0.33053356],\n",
              "        [ 0.45857352],\n",
              "        [-0.21608715],\n",
              "        [-0.26672915],\n",
              "        [-0.34050804],\n",
              "        [-0.4891492 ],\n",
              "        [-0.28343278],\n",
              "        [-0.47893608],\n",
              "        [-0.29147476],\n",
              "        [-0.48909828],\n",
              "        [ 0.49386355]], dtype=float32),\n",
              " array([0.02510072], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWO6jhr57Kse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63fad31e-0b1c-455d-fb93-52b7e48d0618"
      },
      "source": [
        "model.load_weights(filepath=sentiment_classifier_dir+ '/weights.02.hdf5') ## Replacing final epoch weights with 2nd epoch weights\n",
        "model.get_weights()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.02631288,  0.00512799, -0.01612261, ...,  0.0246463 ,\n",
              "         -0.02303137, -0.01013909],\n",
              "        [-0.04053509,  0.00023309,  0.03643062, ..., -0.01560362,\n",
              "         -0.02013558, -0.02105451],\n",
              "        [-0.01018512,  0.00718964, -0.00314832, ..., -0.00703433,\n",
              "         -0.01741731,  0.00388084],\n",
              "        ...,\n",
              "        [-0.00648206, -0.02603475,  0.04113287, ...,  0.02193984,\n",
              "          0.02229369, -0.02997793],\n",
              "        [-0.04690448,  0.01797942, -0.01760292, ...,  0.04302412,\n",
              "          0.05760852,  0.03347955],\n",
              "        [ 0.00710724, -0.0511138 ,  0.00722806, ..., -0.00782971,\n",
              "          0.07346104,  0.03286441]], dtype=float32),\n",
              " array([[ 0.03671942,  0.02312078, -0.03177656, ..., -0.01908625,\n",
              "          0.01509066,  0.00792759],\n",
              "        [-0.03096507,  0.05635709, -0.03369057, ..., -0.02468538,\n",
              "          0.0327282 ,  0.01355584],\n",
              "        [ 0.01437219,  0.00377689, -0.01104469, ..., -0.02745539,\n",
              "         -0.02869383, -0.02195213],\n",
              "        ...,\n",
              "        [ 0.07159834,  0.09537324,  0.01825148, ..., -0.09121143,\n",
              "         -0.02463872,  0.06383973],\n",
              "        [-0.02880725, -0.01304579, -0.04981399, ...,  0.01261201,\n",
              "          0.02464149, -0.03411712],\n",
              "        [ 0.00380074, -0.08212909, -0.02199496, ...,  0.0822112 ,\n",
              "         -0.01636914, -0.06239994]], dtype=float32),\n",
              " array([ 0.02241437,  0.00929846,  0.01347242,  0.01657278,  0.0162653 ,\n",
              "        -0.00823376, -0.01183009,  0.01908422,  0.01854679,  0.02188549,\n",
              "         0.02654946,  0.02610737,  0.01869587,  0.01218117,  0.01228467,\n",
              "         0.02103925,  0.01725581,  0.01276447,  0.01358952, -0.01039094,\n",
              "         0.0217618 , -0.01673329,  0.01495324,  0.02207224,  0.00890537,\n",
              "         0.00984699,  0.01639904,  0.01384714,  0.01598823,  0.0201881 ,\n",
              "         0.01495206,  0.02552398,  0.00994928,  0.01755018,  0.02329026,\n",
              "         0.02454365,  0.00065386,  0.02268155, -0.01306519,  0.0127294 ,\n",
              "         0.01874178,  0.01341387,  0.01947172,  0.02451533,  0.02485178,\n",
              "         0.02077758,  0.01822097,  0.01871025,  0.00946767,  0.01956429,\n",
              "         0.02176044,  0.01420438,  0.00508233,  0.01670082,  0.01014987,\n",
              "        -0.02114687,  0.03118678,  0.02294892,  0.00113416, -0.02131186,\n",
              "         0.0125522 ,  0.01671862,  0.0157645 ,  0.0116141 ], dtype=float32),\n",
              " array([[ 2.1901241e-01],\n",
              "        [ 1.8693072e-01],\n",
              "        [-2.7245498e-01],\n",
              "        [-2.8809059e-01],\n",
              "        [-1.9429766e-01],\n",
              "        [-4.1349895e-02],\n",
              "        [ 3.4547511e-02],\n",
              "        [-2.3791535e-01],\n",
              "        [-1.6650370e-01],\n",
              "        [ 2.8146985e-01],\n",
              "        [ 1.9138420e-01],\n",
              "        [ 1.7783339e-01],\n",
              "        [-3.3891073e-01],\n",
              "        [-3.8743320e-01],\n",
              "        [ 3.8644174e-01],\n",
              "        [-1.4739151e-01],\n",
              "        [-3.1596601e-01],\n",
              "        [ 2.6143458e-01],\n",
              "        [ 2.8092384e-01],\n",
              "        [-1.5643032e-04],\n",
              "        [-1.3299239e-01],\n",
              "        [ 6.1956707e-02],\n",
              "        [ 2.9139918e-01],\n",
              "        [ 2.3282534e-01],\n",
              "        [-1.9038789e-01],\n",
              "        [ 3.1371015e-01],\n",
              "        [ 1.5674734e-01],\n",
              "        [ 1.1153517e-01],\n",
              "        [-2.2455800e-01],\n",
              "        [ 2.6998591e-01],\n",
              "        [ 2.6445219e-01],\n",
              "        [-1.8770298e-01],\n",
              "        [-3.3412498e-01],\n",
              "        [-2.9630187e-01],\n",
              "        [-2.3743406e-01],\n",
              "        [-3.1534013e-01],\n",
              "        [ 1.5381077e-01],\n",
              "        [ 1.5785921e-01],\n",
              "        [-2.7857473e-01],\n",
              "        [-3.7957036e-01],\n",
              "        [ 3.3978072e-01],\n",
              "        [ 4.2609787e-01],\n",
              "        [ 2.2778653e-01],\n",
              "        [ 9.9110954e-02],\n",
              "        [ 1.8309310e-01],\n",
              "        [-2.1852179e-01],\n",
              "        [ 2.7754700e-01],\n",
              "        [-4.1942543e-01],\n",
              "        [-3.6540604e-01],\n",
              "        [-3.4647572e-01],\n",
              "        [ 8.2307711e-02],\n",
              "        [ 2.7574310e-01],\n",
              "        [-9.4252340e-02],\n",
              "        [-2.1375996e-01],\n",
              "        [ 2.7339199e-01],\n",
              "        [-2.0587327e-01],\n",
              "        [-1.7372915e-01],\n",
              "        [-2.2936839e-01],\n",
              "        [-2.9655865e-01],\n",
              "        [-1.3533670e-01],\n",
              "        [-3.4797722e-01],\n",
              "        [-1.9656232e-01],\n",
              "        [-3.2249427e-01],\n",
              "        [ 4.0227440e-01]], dtype=float32),\n",
              " array([0.0182179], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiicM6ff7KxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af460be-7fa2-4060-d47c-d4f0c9d7ce0d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "weights = model.get_weights()\n",
        "\n",
        "\"\"\"\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "embedding (Embedding)        (None, 100, 64)           320000    \n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 6400)              0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 64)                409664    \n",
        "_________________________________________________________________\n",
        "dropout (Dropout)            (None, 64)                0         \n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 1)                 65        \n",
        "=================================================================\n",
        "\"\"\"\n",
        "\n",
        "###Total Layers\n",
        "print(len(weights))\n",
        "##weights in layer 1\n",
        "print(len(weights[0]), np.shape(weights[0]), len(weights[0][0]))\n",
        "##weights in layer 2\n",
        "print(len(weights[1]), np.shape(weights[1]))\n",
        "##weights in layer 3\n",
        "print(len(weights[2]), np.shape(weights[2]))\n",
        "##weights in layer 4\n",
        "print(len(weights[3]), np.shape(weights[3]))\n",
        "##weights in layer 5\n",
        "print(len(weights[4]), np.shape(weights[4]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "5000 (5000, 64) 64\n",
            "6400 (6400, 64)\n",
            "64 (64,)\n",
            "64 (64, 1)\n",
            "1 (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfoajXAN7KzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c7cf58-4e62-4c2b-bcaf-0fec636872c3"
      },
      "source": [
        "#y_hat = model.predict_proba(x_valid) #`model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
        "#print(len(y_hat), y_hat[:10])\n",
        "y_hat = model.predict(x_valid)\n",
        "print(len(y_hat), y_hat[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 [[0.16721259]\n",
            " [0.98007685]\n",
            " [0.8590109 ]\n",
            " [0.71618176]\n",
            " [0.9946903 ]\n",
            " [0.83851653]\n",
            " [0.7961585 ]\n",
            " [0.01112744]\n",
            " [0.9378773 ]\n",
            " [0.87965727]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN15dOXT7K3G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "3d9b904f-1fd0-4d36-a0ca-2caa2dfc3f3a"
      },
      "source": [
        "plt.hist(y_hat)\n",
        "_ = plt.axvline(x=0.5, color = 'yellow')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA1pcsUcv6ZY",
        "outputId": "f19b9e7f-c822-4b2c-c5f0-f570a647e9d9"
      },
      "source": [
        "percentage_auc = roc_auc_score(y_true=y_valid, y_score=y_hat) * 100.0\n",
        "print('percentage_auc == {} % '.format(percentage_auc))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "percentage_auc == 92.81900416000002 % \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "uASeqtFjv6cy",
        "outputId": "d0ad6a3d-8ecd-412f-9b88-2d3c26018e48"
      },
      "source": [
        "y_df = pd.DataFrame(list(zip(y_valid, [yhat[0] for yhat in y_hat])), columns=['y_true', 'y_predict'])\n",
        "y_df.head(10)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_true</th>\n",
              "      <th>y_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.167213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.980077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.859011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.716182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.994690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.838517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0.796158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0.011127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0.937877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>0.879657</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   y_true  y_predict\n",
              "0       0   0.167213\n",
              "1       1   0.980077\n",
              "2       1   0.859011\n",
              "3       0   0.716182\n",
              "4       1   0.994690\n",
              "5       1   0.838517\n",
              "6       1   0.796158\n",
              "7       0   0.011127\n",
              "8       0   0.937877\n",
              "9       1   0.879657"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "cjgxAJjyv6fS",
        "outputId": "816e046d-9021-4cdc-f0bd-865c05ab8ff2"
      },
      "source": [
        "y_df[(y_df.y_true == 1) & (y_df.y_predict <= 0.1)]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_true</th>\n",
              "      <th>y_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1</td>\n",
              "      <td>0.045298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>1</td>\n",
              "      <td>0.091825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>1</td>\n",
              "      <td>0.063953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>1</td>\n",
              "      <td>0.098290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>1</td>\n",
              "      <td>0.052230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>1</td>\n",
              "      <td>0.055661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24783</th>\n",
              "      <td>1</td>\n",
              "      <td>0.099237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24789</th>\n",
              "      <td>1</td>\n",
              "      <td>0.074870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24863</th>\n",
              "      <td>1</td>\n",
              "      <td>0.041039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24985</th>\n",
              "      <td>1</td>\n",
              "      <td>0.088984</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>263 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       y_true  y_predict\n",
              "101         1   0.045298\n",
              "248         1   0.091825\n",
              "300         1   0.063953\n",
              "322         1   0.098290\n",
              "333         1   0.052230\n",
              "...       ...        ...\n",
              "24780       1   0.055661\n",
              "24783       1   0.099237\n",
              "24789       1   0.074870\n",
              "24863       1   0.041039\n",
              "24985       1   0.088984\n",
              "\n",
              "[263 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "YRwZEqksv6kR",
        "outputId": "159e3408-9e56-4aa1-936d-1d7d7055b93a"
      },
      "source": [
        "' '.join(index_word[id] for id in imdb_dataset_actual[1][0][322])    #[True ==1 , Predict ==0]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"START this is easily one of the best movies of the 1950s otto preminger directed only four or five really good movies and this is one of them frank sinatra gives his best performance and the music score by elmer bernstein is dynamite from the opening titles by saul bass to the hysteria of drug addict frank going cold turkey this is a riveting movie with kim novak giving a very good performance eleanor parker giving a very bad performance as well as darren mcgavin as the reptilian pusher and arnold as frank's grifter pal beware of bad prints this movie is in the public domain so some copies are pretty rough\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CuNt1Fptv6h-",
        "outputId": "81daa46e-1afc-41bb-a011-859d3d219634"
      },
      "source": [
        "y_df[(y_df.y_true == 0) & (y_df.y_predict > 0.9)]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_true</th>\n",
              "      <th>y_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0.937877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>0</td>\n",
              "      <td>0.924882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>0</td>\n",
              "      <td>0.970040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>0</td>\n",
              "      <td>0.957741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>0</td>\n",
              "      <td>0.928733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24692</th>\n",
              "      <td>0</td>\n",
              "      <td>0.970115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24696</th>\n",
              "      <td>0</td>\n",
              "      <td>0.977577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24771</th>\n",
              "      <td>0</td>\n",
              "      <td>0.903670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24814</th>\n",
              "      <td>0</td>\n",
              "      <td>0.945858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24826</th>\n",
              "      <td>0</td>\n",
              "      <td>0.928266</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>436 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       y_true  y_predict\n",
              "8           0   0.937877\n",
              "112         0   0.924882\n",
              "256         0   0.970040\n",
              "386         0   0.957741\n",
              "390         0   0.928733\n",
              "...       ...        ...\n",
              "24692       0   0.970115\n",
              "24696       0   0.977577\n",
              "24771       0   0.903670\n",
              "24814       0   0.945858\n",
              "24826       0   0.928266\n",
              "\n",
              "[436 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "McNSiPvt0cpo",
        "outputId": "24157ca8-9823-40fa-cf58-708fc7170cab"
      },
      "source": [
        "' '.join(index_word[id] for id in imdb_dataset_actual[1][0][24826])   #[True ==0 , Predict ==1]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"START i suppose for 1961 this film was supposed to be cool but looking back now 45 years it's charm was just as silly as it's entertainment value granted the special effects do well on t v with the series that started in 1964 but for the big screen i once had a fish tank that was equally as exciting i must agree about the octopus scene near the end where it attached itself to the obviously not well staged or trained overall it's pretty bad acting with shoddy special effects and i still do recommend it for fun laughs sake this was probably one of irwin allen's biggest films and i think he thought a lot of it eden went on to play genie on t v micheal was her husband now that is a cool part about this film i always enjoyed seeing real life husband and wife teams star in the same movie neat\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}